{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "5751d7c496c272386542b26abd80dd065779a39feb74bf22f2b4e5e7f629ef29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### General Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import glob\n",
    "from cleanUp import cleanUp\n",
    "from fillDf import fillDf\n",
    "from fixYearStamp import fixYearStamp\n",
    "from sklearn.cluster import KMeans\n",
    "import time as clock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = clock.time()"
   ]
  },
  {
   "source": [
    "### Data Cleaning\n",
    "Passing the sensor data through the cleanUp function to get fix timestamps and delete null timestamps."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csv_files = glob.glob(\"./Data/*.txt\")\n",
    "# insert the desired start time\n",
    "cutOffTime = '4/20/2021 9:30'\n",
    "endTime = '2021-04-20 14:00'\n",
    "# insert the time rectifying offsets. default of for nothing {'':0}\n",
    "sensorConditions = {'S-01':7,'S-02':7,'S-03':7,'S-04':7,'S-05':7,'S-06':7,'S-15':7,'S-19':7}\n",
    "#This indicates which columns to keep. Here we're taking all of the dP info and the timestamps\n",
    "columns = [0,1,6,7,8,9,10,11]\n",
    "# Enable Data Checking\n",
    "DataChecking = False\n",
    "# Here are obversed timestamps that need to removed from the data\n",
    "badTimes = ['     0/0/0      0:0:0','2165/165/165 165:165:85']\n",
    "# Controls wether zones will be created automatically or by k-means clusters\n",
    "ZoneAutomation = False\n",
    "# Sets either the binning or the manual zones\n",
    "numberOfZones = 4\n",
    "numAutoZones = 3\n",
    "# Sensors to exclude from zone\n",
    "outdoorSensors = ['S-15','S-16','S-18','S-19']\n",
    "# 10s of seconds before nebulization to include in the expirement csv files\n",
    "preCursorFactor = 6\n",
    "# which particle to analyze\n",
    "particle = 'Dp>0.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "expTRange = {\n",
    "\n",
    "    'ICU Room 1 Door Partially Open':\n",
    "    [pd.Timestamp('2021-04-20 9:45:15'),\n",
    "    pd.Timestamp('2021-04-20 10:02:40'),\n",
    "    pd.Timestamp('2021-04-20 10:19:40')],\n",
    "    'ICU Room 1 Door Open':\n",
    "    [pd.Timestamp('2021-04-20 10:35:05'),\n",
    "    pd.Timestamp('2021-04-20 10:51:15'),\n",
    "    pd.Timestamp('2021-04-20 11:06:30')],\n",
    "    'ICU Room 1 Negative Pressure':\n",
    "    [pd.Timestamp('2021-04-20 11:25:00'),\n",
    "    pd.Timestamp('2021-04-20 11:37:50'),\n",
    "    pd.Timestamp('2021-04-20 11:47:55')],\n",
    "    'ICU Room 2 Door Partially Open':\n",
    "    [pd.Timestamp('2021-04-20 12:13:35'),\n",
    "    pd.Timestamp('2021-04-20 12:23:30')+pd.Timedelta(140,'S'),\n",
    "    pd.Timestamp('2021-04-20 12:38:30')+pd.Timedelta(190,'S'),\n",
    "    pd.Timestamp('2021-04-20 12:49:45')+pd.Timedelta(190,'S')],\n",
    "    'ICU Room 2 Door Open':\n",
    "    [pd.Timestamp('2021-04-20 13:00:30')+pd.Timedelta(190,'S'),\n",
    "    pd.Timestamp('2021-04-20 13:13:30'),\n",
    "    pd.Timestamp('2021-04-20 13:23:30'),\n",
    "    pd.Timestamp('2021-04-20 13:33:00')],\n",
    "}\n",
    "\n",
    "#enter in the expirement length as seconds/10\n",
    "expTLen = {\n",
    "    'ICU Room 1 Door Partially Open' : 15*6,\n",
    "    'ICU Room 1 Door Open':15*6,\n",
    "    'ICU Room 1 Negative Pressure':10*6,\n",
    "    'ICU Room 2 Door Partially Open':10*6,\n",
    "    'ICU Room 2 Door Open':10*6   \n",
    "}\n",
    "# Manual Zone set up notice how we are missing S-14\n",
    "zoneList = {\n",
    "    'Zone 1' : ['S-01','S-04'],\n",
    "    'Zone 2' : ['S-02','S-03','S-05','S-06'],\n",
    "    'Zone 3' : ['S-07','S-08','S-09','S-10','S-12','S-13','S-14'], #took out 'S-11' as its data file is missing rn\n",
    "    'Zone 4' : ['S-15','S-18'],\n",
    "    'Zone 5' : ['S-16','S-19']\n",
    "}\n",
    "if not ZoneAutomation:\n",
    "    numberOfZones = len(zoneList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./Data\\\\S-01.txt',\n",
       " './Data\\\\S-02.txt',\n",
       " './Data\\\\S-03.txt',\n",
       " './Data\\\\S-04.txt',\n",
       " './Data\\\\S-05.txt',\n",
       " './Data\\\\S-06.txt',\n",
       " './Data\\\\S-07.txt',\n",
       " './Data\\\\S-08.txt',\n",
       " './Data\\\\S-09.txt',\n",
       " './Data\\\\S-10.txt',\n",
       " './Data\\\\S-12.txt',\n",
       " './Data\\\\S-13.txt',\n",
       " './Data\\\\S-14.txt',\n",
       " './Data\\\\S-15.txt',\n",
       " './Data\\\\S-16.txt',\n",
       " './Data\\\\S-18.txt',\n",
       " './Data\\\\S-19.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "all_csv_files"
   ]
  },
  {
   "source": [
    "Changed this to markdown so it won't run twice, had to fix the timestamps on S-12\n",
    "filePath        = all_csv_files[11]\n",
    "incorrectString = '21/3/22'\n",
    "date            = '3/22/2021'\n",
    "charTimeStart   = 11\n",
    "charTimeEnd     = 21\n",
    "offset          = 0\n",
    "fixYearStamp(filePath,incorrectString,date,charTimeStart,charTimeEnd,offset)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "S-01     2021-04-20 09:30:00      2021-04-20 13:45:59       mod: yes\nS-02     2021-04-20 09:30:00      2021-04-20 13:46:00       mod: yes\nS-03     2021-04-20 09:30:00      2021-04-20 13:45:19       mod: yes\nS-04     2021-04-20 09:30:09      2021-04-20 13:49:09       mod: yes\nS-05     2021-04-20 09:30:00      2021-04-20 13:43:50       mod: yes\nS-06     2021-04-20 09:30:00      2021-04-20 13:44:09       mod: yes\nS-07     2021-04-20 09:30:07      2021-04-20 13:45:27       mod: no\nS-08     2021-04-20 09:30:09      2021-04-20 13:47:03       mod: no\nS-09     2021-04-20 09:30:19      2021-04-20 13:44:35       mod: no\nS-10     2021-04-20 09:30:07      2021-04-20 13:44:23       mod: no\nS-12     2021-04-20 09:30:14      2021-04-20 13:45:15       mod: no\nS-13     2021-04-20 09:30:08      2021-04-20 13:44:38       mod: no\nS-14     2021-04-20 09:30:07      2021-04-20 13:43:57       mod: no\nS-15     2021-04-20 09:30:03      2021-04-20 13:48:44       mod: yes\nS-16     2021-04-20 09:31:17      2021-04-20 13:49:17       mod: no\nS-18     2021-04-20 09:30:08      2021-04-20 13:48:39       mod: no\nS-19     2021-04-20 09:30:02      2021-04-20 13:49:12       mod: yes\n"
     ]
    }
   ],
   "source": [
    "data = cleanUp(cutOffTime,sensorConditions,all_csv_files,columns,badTimes)"
   ]
  },
  {
   "source": [
    "### Exporting Data\n",
    "Here we can export the organized data frames as csv files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './proccessedData'\n",
    "for x in data:\n",
    "    temp=data[x]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Checking Data\n",
    "Here we scan through the data for irregularities in data recording."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DataChecking:\n",
    "    directory = './dataInfo'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    fout = open('./dataInfo/time_Frequency_Error_Log.txt','wt')\n",
    "    errors = {}\n",
    "    errorCount = {}\n",
    "    # Enter the expected interval here\n",
    "    interval = 10\n",
    "    for x in data:\n",
    "        # errors keeps track of length of each time interval error that occurs\n",
    "        errors[x] = set(())\n",
    "        # errorCount keeps track of how many times each time interval error occured\n",
    "        errorCount[x] = {}\n",
    "        # counter keeps track of the total time interval errors per sensor\n",
    "        counter = 0\n",
    "        #shows the total\n",
    "        temp = data[x]\n",
    "        for idx,i in enumerate(temp['Date_Time']):\n",
    "            try:\n",
    "                if not ((temp['Date_Time'][idx+1] - i) == pd.Timedelta(seconds=interval)):\n",
    "                    timeErr = temp['Date_Time'][idx+1] - i\n",
    "                    if str(timeErr.seconds) in errorCount[x]:\n",
    "                        errorCount[x][str(timeErr.seconds)] +=1\n",
    "                    else:\n",
    "                        errorCount[x][str(timeErr.seconds)] = 1\n",
    "\n",
    "                    errors[x].add(timeErr)\n",
    "\n",
    "\n",
    "                    counter += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(str(round(counter/len(temp)*100,2)),'% potential error in ', x)\n",
    "        fout.write('potential error in '+ x +'\\n' + str(round(counter/len(temp)*100,2))+'%'+'\\n')\n",
    "\n",
    "        # display the different types of errors\n",
    "        lst = [i.seconds for i in errors[x]]\n",
    "        frmt = \"{:>4}\"*len(lst)\n",
    "        print(frmt.format(*lst))\n",
    "        fout.write(\"Time Errors\" + frmt.format(*lst)+ '\\n')\n",
    "\n",
    "        # display the quantity of each type of error\n",
    "        lst = [errorCount[x][str(i.seconds)] for i in errors[x]]\n",
    "        frmt = \"{:>4}\"*len(lst)\n",
    "        print(frmt.format(*lst))\n",
    "        fout.write(\"# Observed \" + frmt.format(*lst)+ '\\n')\n",
    "\n",
    "        print()\n",
    "        fout.write('\\n')\n",
    "\n",
    "\n",
    "    fout.close()"
   ]
  },
  {
   "source": [
    "Notice there are quite a few repeating errors here in our data set. We can either choose to interpolate the data inbetween or pad it with 0s. For gaps <40s i will interpolate, but for gaps >40 i will 0 pad."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "S-01   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.0', '% of values not changed : 100.0']\n",
      "S-02   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.0', '% of values not changed : 100.0']\n",
      "S-03   ['% of values from interpolation : 0.131', '% of values from 0-padding : 0.0', '% of values not changed : 99.869']\n",
      "S-04   ['% of values from interpolation : 0.257', '% of values from 0-padding : 0.579', '% of values not changed : 99.164']\n",
      "S-05   ['% of values from interpolation : 0.656', '% of values from 0-padding : 10.433', '% of values not changed : 88.911']\n",
      "S-06   ['% of values from interpolation : 0.131', '% of values from 0-padding : 0.0', '% of values not changed : 99.869']\n",
      "S-07   ['% of values from interpolation : 33.268', '% of values from 0-padding : 0.0', '% of values not changed : 66.732']\n",
      "S-08   ['% of values from interpolation : 65.911', '% of values from 0-padding : 0.778', '% of values not changed : 33.312']\n",
      "S-09   ['% of values from interpolation : 66.688', '% of values from 0-padding : 0.0', '% of values not changed : 33.312']\n",
      "S-10   ['% of values from interpolation : 66.339', '% of values from 0-padding : 0.327', '% of values not changed : 33.333']\n",
      "S-12   ['% of values from interpolation : 66.971', '% of values from 0-padding : 33.029', '% of values not changed : 0.0']\n",
      "S-13   ['% of values from interpolation : 33.246', '% of values from 0-padding : 0.0', '% of values not changed : 66.754']\n",
      "S-14   ['% of values from interpolation : 61.483', '% of values from 0-padding : 8.661', '% of values not changed : 29.856']\n",
      "S-15   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.0', '% of values not changed : 100.0']\n",
      "S-16   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.514', '% of values not changed : 99.486']\n",
      "S-18   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.387', '% of values not changed : 99.613']\n",
      "S-19   ['% of values from interpolation : 0.0', '% of values from 0-padding : 0.0', '% of values not changed : 100.0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fout = open('./dataInfo/interpolation_Effect_Log.txt','wt')\n",
    "interpDF = {}\n",
    "\n",
    "for x in data:\n",
    "    df = data[x]\n",
    "    cutoff = 40\n",
    "    freq = '10S'\n",
    "    try:\n",
    "        interpDF[x],accuracy = fillDf(df,freq,cutOffTime,endTime,cutoff)\n",
    "        print(x,' ',accuracy)\n",
    "        fout.write(x+' '+ '\\n' + accuracy[0]+ '\\n'+ accuracy[1]+ '\\n'+ accuracy[2] +'\\n\\n')\n",
    "    except IndexError:\n",
    "        print(x,'NO DATA')\n",
    "        fout.write(x+'NO DATA'+'\\n')\n",
    "fout.close()        "
   ]
  },
  {
   "source": [
    "### Export Data\n",
    "export the newly interpolated data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './interpolatedData'\n",
    "for x in interpDF:\n",
    "    temp=interpDF[x]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Merge the DataFrames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4 1524\n"
     ]
    }
   ],
   "source": [
    "length = []\n",
    "for x in interpDF:\n",
    "    length.append(len(interpDF[x]))\n",
    "index = min(length)\n",
    "lowIDX,lowValue = [[i,value] for i,value in enumerate(length) if value == index][0]\n",
    "print(lowIDX,lowValue)"
   ]
  },
  {
   "source": [
    "for count,key in enumerate(list(interpDF.keys())):\n",
    "    print(count+1,key,temp[count+1])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               Date_Time  S-01  S-02  S-03  S-04  S-05  S-06  S-07  S-08  \\\n",
       "0    2021-04-20 09:30:00    69    39    54    36    27    18   219   120   \n",
       "1    2021-04-20 09:30:10    48    93    54    84    18    81   147    81   \n",
       "2    2021-04-20 09:30:20    39    81    48    72    48    99    90    81   \n",
       "3    2021-04-20 09:30:30     9    48    69    39    48    36   126   205   \n",
       "4    2021-04-20 09:30:40    27    39    27    48    42    18    81   591   \n",
       "...                  ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "1519 2021-04-20 13:43:10    21     9     9     0    36     0    18     9   \n",
       "1520 2021-04-20 13:43:20     9     9     0     0     9     0     9     0   \n",
       "1521 2021-04-20 13:43:30    18     0     0     0     9     0     0     0   \n",
       "1522 2021-04-20 13:43:40     9     0     9     0    18     0     9     0   \n",
       "1523 2021-04-20 13:43:50     9     0     9     0     9     0     9     9   \n",
       "\n",
       "      S-09  S-10  S-12  S-13  S-14  S-15  S-16  S-18  S-19     Average  \\\n",
       "0     1302    21    39    36    63    33     0    42    27  157.153846   \n",
       "1     1302    66    39    72    63    75     0    30    27  165.230769   \n",
       "2      750    66    39    36    45    54     0    57    57  114.923077   \n",
       "3      750    69    72    75    54    21     0    36    57  123.076923   \n",
       "4      427    84   105    75    54    39     0    30    27  124.461538   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...         ...   \n",
       "1519     0    57     4     0     0    18     9     0     0   12.538462   \n",
       "1520    28    28     0    18     4     9     0     0     0    8.769231   \n",
       "1521    66     0     4     9     0     0     0     0    21    8.153846   \n",
       "1522    66     0     9     9     0     0    18   126    30    9.923077   \n",
       "1523    33     0     9    18     0     0     9   126     9    8.076923   \n",
       "\n",
       "           Variance  \n",
       "0     111944.130178  \n",
       "1     108561.408284  \n",
       "2      34026.071006  \n",
       "3      34940.840237  \n",
       "4      28521.940828  \n",
       "...             ...  \n",
       "1519     271.171598  \n",
       "1520      94.792899  \n",
       "1521     307.207101  \n",
       "1522     292.686391  \n",
       "1523      80.840237  \n",
       "\n",
       "[1524 rows x 20 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date_Time</th>\n      <th>S-01</th>\n      <th>S-02</th>\n      <th>S-03</th>\n      <th>S-04</th>\n      <th>S-05</th>\n      <th>S-06</th>\n      <th>S-07</th>\n      <th>S-08</th>\n      <th>S-09</th>\n      <th>S-10</th>\n      <th>S-12</th>\n      <th>S-13</th>\n      <th>S-14</th>\n      <th>S-15</th>\n      <th>S-16</th>\n      <th>S-18</th>\n      <th>S-19</th>\n      <th>Average</th>\n      <th>Variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-04-20 09:30:00</td>\n      <td>69</td>\n      <td>39</td>\n      <td>54</td>\n      <td>36</td>\n      <td>27</td>\n      <td>18</td>\n      <td>219</td>\n      <td>120</td>\n      <td>1302</td>\n      <td>21</td>\n      <td>39</td>\n      <td>36</td>\n      <td>63</td>\n      <td>33</td>\n      <td>0</td>\n      <td>42</td>\n      <td>27</td>\n      <td>157.153846</td>\n      <td>111944.130178</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-04-20 09:30:10</td>\n      <td>48</td>\n      <td>93</td>\n      <td>54</td>\n      <td>84</td>\n      <td>18</td>\n      <td>81</td>\n      <td>147</td>\n      <td>81</td>\n      <td>1302</td>\n      <td>66</td>\n      <td>39</td>\n      <td>72</td>\n      <td>63</td>\n      <td>75</td>\n      <td>0</td>\n      <td>30</td>\n      <td>27</td>\n      <td>165.230769</td>\n      <td>108561.408284</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-04-20 09:30:20</td>\n      <td>39</td>\n      <td>81</td>\n      <td>48</td>\n      <td>72</td>\n      <td>48</td>\n      <td>99</td>\n      <td>90</td>\n      <td>81</td>\n      <td>750</td>\n      <td>66</td>\n      <td>39</td>\n      <td>36</td>\n      <td>45</td>\n      <td>54</td>\n      <td>0</td>\n      <td>57</td>\n      <td>57</td>\n      <td>114.923077</td>\n      <td>34026.071006</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-04-20 09:30:30</td>\n      <td>9</td>\n      <td>48</td>\n      <td>69</td>\n      <td>39</td>\n      <td>48</td>\n      <td>36</td>\n      <td>126</td>\n      <td>205</td>\n      <td>750</td>\n      <td>69</td>\n      <td>72</td>\n      <td>75</td>\n      <td>54</td>\n      <td>21</td>\n      <td>0</td>\n      <td>36</td>\n      <td>57</td>\n      <td>123.076923</td>\n      <td>34940.840237</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021-04-20 09:30:40</td>\n      <td>27</td>\n      <td>39</td>\n      <td>27</td>\n      <td>48</td>\n      <td>42</td>\n      <td>18</td>\n      <td>81</td>\n      <td>591</td>\n      <td>427</td>\n      <td>84</td>\n      <td>105</td>\n      <td>75</td>\n      <td>54</td>\n      <td>39</td>\n      <td>0</td>\n      <td>30</td>\n      <td>27</td>\n      <td>124.461538</td>\n      <td>28521.940828</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1519</th>\n      <td>2021-04-20 13:43:10</td>\n      <td>21</td>\n      <td>9</td>\n      <td>9</td>\n      <td>0</td>\n      <td>36</td>\n      <td>0</td>\n      <td>18</td>\n      <td>9</td>\n      <td>0</td>\n      <td>57</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>18</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12.538462</td>\n      <td>271.171598</td>\n    </tr>\n    <tr>\n      <th>1520</th>\n      <td>2021-04-20 13:43:20</td>\n      <td>9</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>28</td>\n      <td>28</td>\n      <td>0</td>\n      <td>18</td>\n      <td>4</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.769231</td>\n      <td>94.792899</td>\n    </tr>\n    <tr>\n      <th>1521</th>\n      <td>2021-04-20 13:43:30</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>66</td>\n      <td>0</td>\n      <td>4</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21</td>\n      <td>8.153846</td>\n      <td>307.207101</td>\n    </tr>\n    <tr>\n      <th>1522</th>\n      <td>2021-04-20 13:43:40</td>\n      <td>9</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>18</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>66</td>\n      <td>0</td>\n      <td>9</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>18</td>\n      <td>126</td>\n      <td>30</td>\n      <td>9.923077</td>\n      <td>292.686391</td>\n    </tr>\n    <tr>\n      <th>1523</th>\n      <td>2021-04-20 13:43:50</td>\n      <td>9</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>9</td>\n      <td>9</td>\n      <td>33</td>\n      <td>0</td>\n      <td>9</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>126</td>\n      <td>9</td>\n      <td>8.076923</td>\n      <td>80.840237</td>\n    </tr>\n  </tbody>\n</table>\n<p>1524 rows × 20 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "columns = list(interpDF.keys())\n",
    "mergedData = pd.DataFrame({'Date_Time':interpDF[columns[lowIDX]]['Date_Time']})\n",
    "for idx,column in enumerate(columns):\n",
    "    mergedData[column] = interpDF[column][particle]\n",
    "Average = np.mean(mergedData[zoneList['Zone 1']+zoneList['Zone 2']+zoneList['Zone 3']],axis=1)\n",
    "Variance = np.var(mergedData[zoneList['Zone 1']+zoneList['Zone 2']+zoneList['Zone 3']],axis=1)\n",
    "mergedData['Average'] = Average\n",
    "mergedData['Variance'] = Variance\n",
    "mergedData"
   ]
  },
  {
   "source": [
    "### Increase Resolution on mergedData"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in mergedData:\n",
    "    tempFrame = mergedData.values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    hiResMergedDF = pd.DataFrame(tempList, columns = mergedData.keys())"
   ]
  },
  {
   "source": [
    "### Export Merged Frames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './mergedData/'\n",
    "if not os.path.exists(directory):\n",
    "\n",
    "    os.makedirs(directory)\n",
    "\n",
    "location = os.path.join(directory+'mergedFrame.csv')\n",
    "hiResMergedDF.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Create csv files for each animation\n",
    "We have 3 expirements in each that we want to average across the range"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergedData = pd.read_csv('./mergedData/mergedFrame.csv',parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time = mergedData['Date_Time']\n",
    "expIndexes = {}\n",
    "for i in expTRange:\n",
    "    expIndexes[i] = []\n",
    "    for x in expTRange[i]:\n",
    "        for start,n in enumerate(time):\n",
    "           if n >= x:\n",
    "               expIndexes[i].append(start)\n",
    "               break"
   ]
  },
  {
   "source": [
    "## Determining Zones\n",
    "Here we first create 'averagedFrame's. These are dictionaries that at each 'label' (which corresponds to the name of an expirement) we have a pandas dataframe containing the results of all of the trails in an expirement summed, and then divided by the total number of trails.\n",
    "Anytime you are adjusting the Zones, everything below here must be run. The values of many of these DataFrames are mutated"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preCursorFactor is defined at the start\n",
    "averagedFrame = {}\n",
    "expirementFrame = {}\n",
    "\n",
    "for label in expIndexes:\n",
    "    runSumFrames = expIndexes[label][0]-expIndexes[label][0]\n",
    "    for idx,time in enumerate(expIndexes[label]):\n",
    "        start = expIndexes[label][idx] - preCursorFactor\n",
    "        end = expIndexes[label][idx] + expTLen[label]\n",
    "        expirementFrame[label+' Exp '+str(idx+1)] = mergedData.iloc[ start : end , 1: ].reset_index(drop = True)\n",
    "        runSumFrames += expirementFrame[label+' Exp '+str(idx+1)]\n",
    "        \n",
    "    averagedFrame[label] = runSumFrames/(idx+1)"
   ]
  },
  {
   "source": [
    "Calculating the correct Zones for each expirement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# numAutoZones is defined at the start\n",
    "AutoZoneAssignments = {}\n",
    "for frame in averagedFrame:\n",
    "    # at this point averagedFrame should just be the averaged sum of the expirementFrame trails. Last two columns are overall average and varaince so they should be ignored.\n",
    "    avgFrm = averagedFrame[frame]\n",
    "    # outdoorSensors must have its spelling exactly match\n",
    "    columns = list(set(avgFrm.keys()[:-2])- set(outdoorSensors))\n",
    "    columns.sort()\n",
    "\n",
    "    X = {}\n",
    "    for column in columns:\n",
    "        value,index = max([(value,index) for index,value in enumerate(avgFrm[column])]) \n",
    "        X[column] = np.array([np.log(value+.01),index])\n",
    "    X = [X[i] for i in X]\n",
    "    kmeans = KMeans(n_clusters=numAutoZones,random_state=0).fit(X)\n",
    "    idx = np.argsort(kmeans.cluster_centers_.sum(axis=1))\n",
    "    lut = np.zeros_like(idx)\n",
    "    lut[idx] = np.arange(numAutoZones)\n",
    "    #lut = lut[::-1]\n",
    "    orderedZones = [[]]*numAutoZones\n",
    "    for index, zone in enumerate(lut):\n",
    "        orderedZones[index] = [index if zone == kmeans.labels_[i] else 0 for i in range(len(kmeans.labels_))]\n",
    "    AutoZoneAssignments[frame] = np.sum(orderedZones,axis=0)\n",
    "z = numAutoZones\n",
    "ZDfAuto = pd.DataFrame(AutoZoneAssignments)\n",
    "ZDfAuto = ZDfAuto.append(pd.DataFrame([[z]*len(expIndexes)]*len(outdoorSensors),columns = AutoZoneAssignments.keys()),ignore_index=True)\n",
    "AutoZoneAssignments = ZDfAuto\n",
    "if len(outdoorSensors):\n",
    "    numAutoZones += 1\n",
    "\n",
    "if not ZoneAutomation:\n",
    "    ZoneAssignments = {}\n",
    "    for frame in averagedFrame:\n",
    "        # at this point averagedFrame should just be the averaged sum of the expirementFrame trails. Last two columns are overall average and varaince so they should be ignored.\n",
    "        avgFrm = averagedFrame[frame]\n",
    "        # outdoorSensors must have its spelling exactly match\n",
    "        columns = list(set(avgFrm.keys()[:-2]))\n",
    "        columns.sort()\n",
    "        ZoneAssignments[frame] = [0]*len(columns)\n",
    "        for value,zone in enumerate(zoneList):\n",
    "            for sensor in zoneList[zone]:\n",
    "                ZoneAssignments[frame][columns.index(sensor)] = value\n",
    "    ZDf = pd.DataFrame(ZoneAssignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './dataInfo'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "location = os.path.join(directory,'ZoneAssignments.csv')\n",
    "ZDf.to_csv(location,index=False)\n",
    "\n",
    "directory = './dataInfo'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "location = os.path.join(directory,'AutoZoneAssignments.csv')\n",
    "ZDfAuto.to_csv(location,index=False)\n",
    "\n",
    "import copy\n",
    "expirementFrameAuto = copy.deepcopy(expirementFrame)\n",
    "averagedFrameAuto = copy.deepcopy(averagedFrame)"
   ]
  },
  {
   "source": [
    "Zoning the expirement data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Zoning the Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonedAvgFrame = {}\n",
    "for key in ZoneAssignments:\n",
    "    occourances = [list(ZoneAssignments[key]).count(x) for x in set(ZoneAssignments[key])]\n",
    "    zoneRunSum = [0]*numberOfZones\n",
    "    zonedAvgFrame[key] = averagedFrame[key]\n",
    "    for idx,column in enumerate(columns):\n",
    "        zoneRunSum[ZoneAssignments[key][idx]] += zonedAvgFrame[key][column]\n",
    "    for idx in range(numberOfZones):\n",
    "        zonedAvgFrame[key]['Zone '+str(idx+1)] = zoneRunSum[idx]/occourances[idx]\n",
    "\n",
    "# relies on columns still being the values of S-01 - last sensor\n",
    "\n",
    "# Declare an empty dictionary for storing the averaged data for each expirement at the end\n",
    "zonedExpFrame = {}\n",
    "# create a list of all of the various dict keys in expirementFrame so that we can iterate through them to get the data\n",
    "labels = list(expirementFrame.keys())\n",
    "# Take the labels list and remove the Exp # from it, so that now we have a list of keys that we can use to correctly save to create correctly corresponding keys for a dictionary that will store the averages\n",
    "keyList = [x.split(' Exp')[0] for x in labels]\n",
    "\n",
    "for index,exp in enumerate(labels):\n",
    "    # set the key variable to correspond to the exp variable\n",
    "    key = keyList[index]\n",
    "    # Create a runnning sum to keep track of the values\n",
    "    zoneRunSum = [0]*numberOfZones\n",
    "    # set the give the zoneExpFrame the same \n",
    "    zonedExpFrame[exp] = expirementFrame[exp]\n",
    "    occourances = [list(ZoneAssignments[key]).count(x) for x in set(ZoneAssignments[key])]\n",
    "    for idx,column in enumerate(columns):\n",
    "        zoneRunSum[ZoneAssignments[key][idx]] += zonedExpFrame[exp][column]\n",
    "    for idx in range(numberOfZones):\n",
    "        zonedExpFrame[exp]['Zone '+str(idx+1)] = zoneRunSum[idx]/occourances[idx]\n",
    "\n",
    "        \n",
    "zonedAvgFrameAuto = {}\n",
    "for key in AutoZoneAssignments:\n",
    "    occourances = [list(AutoZoneAssignments[key]).count(x) for x in set(AutoZoneAssignments[key])]\n",
    "    zoneRunSum = [0]*numAutoZones\n",
    "    zonedAvgFrameAuto[key] = averagedFrameAuto[key]\n",
    "    for idx,column in enumerate(columns):\n",
    "        zoneRunSum[AutoZoneAssignments[key][idx]] += zonedAvgFrameAuto[key][column]\n",
    "    for idx in range(numAutoZones):\n",
    "        zonedAvgFrameAuto[key]['Zone '+str(idx+1)] = zoneRunSum[idx]/occourances[idx]\n",
    "        \n",
    "# relies on columns still being the values of S-01 - last sensor\n",
    "\n",
    "# Declare an empty dictionary for storing the averaged data for each expirement at the end\n",
    "zonedExpFrameAuto = {}\n",
    "# create a list of all of the various dict keys in expirementFrameAuto so that we can iterate through them to get the data\n",
    "labels = list(expirementFrameAuto.keys())\n",
    "# Take the labels list and remove the Exp # from it, so that now we have a list of keys that we can use to correctly save to create correctly corresponding keys for a dictionary that will store the averages\n",
    "keyList = [x.split(' Exp ')[0] for x in labels]\n",
    "\n",
    "for index,exp in enumerate(labels):\n",
    "    # set the key variable to correspond to the exp variable\n",
    "    key = keyList[index]\n",
    "    # Create a runnning sum to keep track of the values\n",
    "    zoneRunSum = [0]*numAutoZones\n",
    "    # set the give the zoneExpFrame the same \n",
    "    zonedExpFrameAuto[exp] = expirementFrameAuto[exp]\n",
    "    occourances = [list(AutoZoneAssignments[key]).count(x) for x in set(AutoZoneAssignments[key])]\n",
    "    for idx,column in enumerate(columns):\n",
    "        zoneRunSum[AutoZoneAssignments[key][idx]] += zonedExpFrameAuto[exp][column]\n",
    "    for idx in range(numAutoZones):\n",
    "        zonedExpFrameAuto[exp]['Zone '+str(idx+1)] = zoneRunSum[idx]/occourances[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './averagedData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in averagedFrame:\n",
    "    temp=averagedFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './averagedDataAuto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in averagedFrameAuto:\n",
    "    temp=averagedFrameAuto[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './expirementData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in expirementFrame:\n",
    "    temp=expirementFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './expirementDataAuto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in expirementFrame:\n",
    "    temp=expirementFrame[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "source": [
    "### Increase the Resolution\n",
    "pad out the dataframes to have values for every second."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretchedDF = {}\n",
    "for i in averagedFrame:\n",
    "    tempFrame = averagedFrame[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchedDF[i] = pd.DataFrame(tempList, columns = expirementFrame[list(expirementFrame.keys())[0]].columns)\n",
    "\n",
    "stretchExpDf = {}\n",
    "for i in expirementFrame:\n",
    "    tempFrame = expirementFrame[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchExpDf[i] = pd.DataFrame(tempList, columns = expirementFrame[list(expirementFrame.keys())[0]].columns) \n",
    "stretchedDFAuto = {}\n",
    "for i in averagedFrameAuto:\n",
    "    tempFrame = averagedFrameAuto[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchedDFAuto[i] = pd.DataFrame(tempList, columns = expirementFrameAuto[list(expirementFrameAuto.keys())[0]].columns) \n",
    "\n",
    "stretchExpDfAuto = {}\n",
    "for i in expirementFrameAuto:\n",
    "    tempFrame = expirementFrameAuto[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchExpDfAuto[i] = pd.DataFrame(tempList, columns = expirementFrameAuto[list(expirementFrameAuto.keys())[0]].columns)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './stretchedAvgData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchedDF:\n",
    "    temp=stretchedDF[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './stretchedExpirementData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchExpDf:\n",
    "    temp=stretchExpDf[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './stretchedAvgDataAuto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchedDFAuto:\n",
    "    temp=stretchedDFAuto[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './stretchedExpirementDataAuto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchExpDfAuto:\n",
    "    temp=stretchExpDfAuto[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29.608975410461426\n"
     ]
    }
   ],
   "source": [
    "end = clock.time()\n",
    "print(end-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}